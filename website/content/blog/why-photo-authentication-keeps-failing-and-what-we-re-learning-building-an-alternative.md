---
title: Why Photo Authentication Keeps Failing (And What We're Learning Building an Alternative)
date: 2026-02-12T16:00:00
author: Samuel C. Ryan
excerpt: |-
  The Verge: "95% of authenticated images lose their metadata when shared on social media." C2PA credentials get stripped by Instagram, Facebook, Twitter during upload.
  When metadata-based authentication fails, what architecture works instead? We're testing registry-based verification that survives platform compression.
image: /images/blog/ff5ef42b-9cfb-4ef9-847c-cc0c4debf202.png
---

When Reuters had to pull AI-generated images from their wire service in 2024, they identified the fakes using Adobe's Content Credentials. The system worked—in that controlled workflow, with preserved metadata, in a professional newsroom. But The Verge just published an [interview with reporter Jess Weatherbed](https://www.theverge.com/decoder-podcast-with-nilay-patel) documenting why this approach fails everywhere else. Her conclusion after months covering C2PA adoption: "there is not going to be a point in the next three, five years where we sign on and go, 'I can now tell what's real and what's not because of C2PA.'" The problem isn't cryptography—it's that 95% of authenticated images lose their metadata when shared on social media. Instagram, Facebook, Twitter all strip EXIF data during upload. C2PA credentials get destroyed along with everything else. OpenAI, which sits on C2PA's steering committee, openly admits their implementation is "incredibly easy to strip."

There's also a structural issue Weatherbed identifies: verification infrastructure controlled by corporations with commercial incentives that conflict with truth. Adobe controls Content Credentials, platforms gate API access, terms change. She quotes journalists being "hesitant to depend on corporate-controlled infrastructure for truth verification." This matches what organizations have told us directly—they want authentication infrastructure they control, not rent from tech companies who can change terms or shut down services.

We're prototyping an approach that separates authentication from file metadata entirely. Instead of embedding credentials in images, cameras submit cryptographic hashes to a public registry at the moment of capture. When someone encounters an image anywhere—after Instagram compression, in screenshots, wherever—they hash what they're looking at and check the registry. Match means it came from an authenticated camera. No match means it didn't. The registry uses consortium blockchain architecture (yes, blockchain—not for crypto ideology but because journalism organizations need infrastructure they operate rather than depend on corporate APIs) with validator nodes run by fact-checking networks and press freedom organizations at roughly $100-150/month per node. We're using camera sensor fingerprints (PRNU patterns) as hardware roots of trust that AI generators can't replicate without physical camera access.

What this solves: proving hardware origin versus AI generation. What it doesn't solve: scene manipulation, staged photos, Photoshop edits after capture. That's a narrower problem than C2PA attempted, which might be why it's solvable. We're currently assembling Phase 1 prototype hardware (Raspberry Pi + camera sensor, arriving this week) to validate the cryptographic pipeline. Phase 2 moves to mobile app implementation—photographers simply take pictures normally while authentication happens invisibly in the background. The submission record is roughly 2KB (a tiny fraction of the image data itself), making the process fast enough to be unnoticeable. Verification happens via browser extension or platform integration for anyone viewing the image later. We're targeting 50-100 beta users in photojournalism and fact-checking communities by Q2 2026, with initial deployment focused on photography competitions where AI submission fraud is immediate and submission/validation processes are already controlled. We're in early discussions with WITNESS and have received positive responses from IFCN, though no formal partnerships yet.

The hardest problems aren't technical. They're adoption (why would photographers change workflows?), governance (who decides what's authenticated and what validator node operators are trusted?), and sustainability (how does registry infrastructure stay funded long-term?). Current discussions focus on governance models where validator node decisions require supermajority consensus among mission-aligned organizations, preventing any single entity from controlling authentication. We're publishing everything openly—[technical specs](https://birthmarkstandard.org/whitepaper), threat models, economic analysis—specifically so these questions get scrutinized before we're too far down a wrong path. Weatherbed's reporting confirms metadata-based authentication has failed. We're testing whether registry-based authentication works any better. Early results in a few months.
